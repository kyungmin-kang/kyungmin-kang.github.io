---
layout: post
title: "The Six Qualities of Good Data"
date: 2025-05-12
excerpt: "Recognizing the most common data pitfalls"
tags: [Data quality]
---


In our last post, we discussed what makes data “good”—**representative**, **low bias**, and **low standard error**. 

In this post, we break it down into six key attributes. Think of these as the pillars that support trustworthy analysis:

1. **Accuracy**
2. **Completeness**
3. **Consistency**
4. **Timeliness**
5. **Relevance**
6. **Reliability**

These aren’t just buzzwords nor are they isolated concepts. Each plays a direct role in making sure our data leads to valid conclusions. **Completeness and consistency** help maintain **representativeness**, preventing data from being skewed by missing or inconsistent values. **Accuracy and reliability** help **reduce bias**, ensuring that data reflects reality. **Timeliness and relevance** ensure that data remains **up-to-date and precise**, helping maintain **low standard error** by filtering out outdated or irrelevant observations.

---

## 1. Accuracy

Accuracy means your data actually reflects reality and the recorded values are accurate.

If someone reports their **monthly salary** when you expected **annual salary**, your entire analysis might be off by a factor of 12. That’s not just noisy—it’s wrong.

Accuracy helps reduce **measurement bias**. To improve it:

- Use validation checks during data entry.
- Ask clear survey questions.
- Cross-reference with trusted sources.

---

## 2. Completeness

Completeness means no important data is missing.

Missing values aren’t just inconvenient. They can introduce **selection bias** or inflate **standard error**.

For example: If lower-paid employees are less likely to respond to a salary survey, your dataset will be *biase upward*.

There are three types of missing data:

- **MCAR (Missing Completely At Random)** – Rare and mostly harmless.
- **MAR (Missing At Random)** – Can be adjusted for.
- **MNAR (Missing Not At Random)** – Needs careful handling and additional assumptions.

We'll cover how to deal with all of these in a future post on missing data.

---

## 3. Consistency

Consistency means the same thing is recorded the same way—across sources, formats, and time. It ensures that data is **structured uniformly**, reducing errors in data ingegration and improving representativeness.

If one file says “Data Scientist” and another says “ML Engineer” but they refer to the same role, that’s a consistency issue.

Or imagine mixing salaries in **USD** and **EUR** without converting. Your results are not meaningful.

Consistency is especially important when merging datasets. Standardize units, labels, and formats before combining data.

---

## 4. Timeliness

Timeliness is about *when* the data was collected and whether it is still relevant.

Outdated data leads to outdated conclusions. Imagine trying to estimate 2024 salaries using data from 2015. That’s nearly a decade of inflation, remote work, AI booms, and layoffs you’re ignoring. Using outdated data can thus lead us to make incorrect conclusions about current job market trends.

Make sure your data reflects the time period you care about. In some domains, this means hours. In others, it means years.

---

## 5. Relevance

Just because you *have* the data doesn’t mean you *need* the data.

Irrelevant variables create noise, slow down analysis, and increase the risk of **overfitting** in models, where models learn patterns that do not generalize beyond the data it is estimated on.

If you're predicting salaries, you probably don’t need someone’s Spotify listening history, but you might need years of experience, company size, or location. Selecting relevant variables is key to ensuring that our dataset is informative rathar than just big.


---

## 6. Reliability

Reliability means the data is trustworthy and can be consistently reproduced.

- Was it collected using sound methodology?
- Are definitions and processes clearly documented?
- Can others replicate the results?

A real-world example of unreliable, and inconsistent data affecting decision-making:  during the early stages of COVID-19, different countries reported cases using different standards. Some included only lab-confirmed cases. Others counted suspected ones based on symptoms. Some countries tested everyone who may have been in contact with someone with COVID symptoms, other countries tested only those who voluntarily took the COVID test. The result? Misleading comparisons—and in some cases, bad policy.

Cross-reference with multiple sources and prefer datasets from organizations with clear, transparent methods.

---

## Why This All Matters

Every one of these six qualities contributes to the broader goals we discussed last time:

- **Accuracy** and **reliability** reduce **bias**.
- **Completeness** and **consistency** improve **representativeness**.
- **Timeliness** and **relevance** help reduce **standard error** and improve decision quality.

---

## Coming Up Next: Sampling Like a Scientist

Great data doesn’t just appear—it’s collected. And how you collect it matters. From choosing a sample to handling missing values, from validating sources to maintaining consistency across datasets, each decision impacts the final results. Thus, we should put emphasis on obtaining high data quality well before actual analysis begins.

In our next post, we’ll look at the science of **sampling**:  
How to design a sample that represents your population, minimizes bias, and sets your project up for success—whether you’re working on a social science study, business forecast, or machine learning model.
