---
layout: post
title: "What Makes Data Good?"
date: 2025-05-09
excerpt: "Data quality Matters"
tags: [Data quality]
---


In 2024, a study titled *Nonstandard Errors* appeared in the Journal of Finance. What's fascinating about the paper: it had 164 coauthors. It asked a simple but unsettling question:  
If you gave **the same dataset** and **the same research question** to different research teams, would they reach the same conclusion?

The answer: **no**. The teams arrived at widely different results.  

This isn’t just a finance problem. Similar findings have surfaced in **operations research**, **economics**, **psychology**, **sociology**, and beyond. It all points to the same truth:

> Even when the data is the same, the way we **process**, **filter**, and **interpret** it can change the outcome.

That means the most powerful part of a data science project may not be the model—-it’s the **data decisions** made before any modeling begins.


---

## Why Data Quality Matters

Before any model is trained, report is built, or recommendation is made, we have to ask:  
**Is the data any good?**

Let’s take an example from business. Imagine an e-commerce company is using historical data to forecast sales:

- If their **sales numbers are inflated**, the forecast will be too optimistic. This would lead to overstocking, which in turn results in increase in unnecessary costs and decreased revenue.
- If their **sales numbers are too low**, the forecast will be too conservative, leading to understocking, which in turn results in lost revenue and profit.

In both cases, the mistake isn’t the model. The model is built on the **flawed foundation**.

Bad data can mislead governments (think: underreported poverty rates), distort healthcare decisions (think: noisy patient records), or even lead to inaccurate drug studies. Good data, on the other hand, provides a solid foundation for **reliable models, sound decisions, and scalable insight**.

---

## So, What *Is* Good Data?

Let’s start with the basic goal of using data, starting from a traditional statistician's point of view:  
> We use data to learn something about the world (usually, a **population**) by looking at a **sample**.

Some computer scientists overlook the sample-population analogy thinking it is irrelevant, but even they use it every data science project. As we'll learn, machine learning hinges on the idea of dividing the entire available data into different subsets, some to study pattern (**train the model**) and another to study how the finding can generalize to data the model has not seen (**validation**). Sample data is used to learn something about the world! 


Then, if that sample is bad or the measurements are flawed, our conclusions won’t hold up.

### This Means Good Data Is:

- **Representative**: It captures the diversity of the population we care about.
- **Low Bias**: It doesn’t systematically lean in one direction (selection or measurement).
- **Low Standard Error**: It has enough volume and variety to avoid noise.

Let’s walk through an example to solidify the understanding.

---

## A Real-World Example: Estimating Salaries

Imagine you want to estimate the **average salary of data scientists** in a major city. How can it go wrong?

### 1. **Selection Bias**  
You collect data from Google, Meta, OpenAI, and other top labs. The average looks great: \$240,000!  
But these aren’t average jobs. These companies tend to offer highly competitive compensation packages. The **sample isn’t representative**. You’ll overestimate.

### 2. **Measurement Bias**  
Let’s say you *do* get a representative sample. But some respondents report total comp (salary + bonus + equity), others only report base salary.  
Now your numbers are **systematically skewed**, even though the sample was right.

### 3. **High Standard Error**  
You collect honest, representative responses… from just 5 people. That’s not enough.  
The **small sample size means high variability**, and any estimate you give is likely to be way off.

But if you fix all three—-representation, accuracy, and volume—-then you’re in a good place. We'll consider this as **good data**.

---

## Data Quality in Machine Learning

Let's briefly revisit our discussion of sample-population analogy. In machine learning:

- You train a model on a **training set** (a sample).
- You tune it on a **validation set** (another sample).
- You test it on a **test set** (yet another sample).

Then you use your model to run on real-world data, making real-world predictions (population). 

If any of these sets are biased, unrepresentative, or too small, your model findings won’t generalize no matter how fancy your algorithm is.

---

## Coming Up Next: The Six Pillars of Good Data

In the next post, we’ll break down the **six key attributes** that make up good data:

- Accuracy
- Completeness
- Consistency
- Timeliness
- Relevance
- Reliability

We’ll show how each one helps reduce bias, improve representativeness, or reduce standard error—and how to spot problems before they derail your project.
